{
  "input": {
    "mode": "Structured",
    "task": "Debug",
    "code": "import os\nimport streamlit as st\nfrom langchain_ollama import ChatOllama\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom datetime import datetime\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\n# ========== Setup Paths ==========\nBASE_DIR = os.path.expanduser(\"~/myworkspace/utilities/code-demo\")\nFILES_DIR = os.path.join(BASE_DIR, \"files\")\nos.makedirs(FILES_DIR, exist_ok=True)\n\n# ========== Initialize Model ==========\nllm = ChatOllama(model=\"codellama:7b-instruct\")\n\n# ========== Custom Styling ==========\nst.markdown(\"\"\"\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap');\n\n    html, body, [class*=\"css\"]  {\n        font-family: 'Montserrat', sans-serif;\n    }\n\n    h1, h2, h3, h4, h5 {\n        color: #2D2042;\n    }\n\n    .stButton > button {\n        background-color: #60B5E5 !important;\n        color: white !important;\n        font-weight: 600;\n        border-radius: 8px;\n    }\n\n    .stSidebar h1, .stSidebar h2, .stSidebar h3, .stSidebar h4, .stSidebar h5 {\n        color: #60B5E5 !important;\n    }\n\n    section[data-testid=\"stFileUploader\"] > div {\n        box-shadow: 0px 1px 5px rgba(0, 0, 0, 0.05);\n        border-radius: 8px;\n        padding: 1rem;\n    }\n\n    #MainMenu {visibility: hidden;}\n    /* footer {visibility: hidden;} */\n    /* header {visibility: hidden;} */\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# ========== Helper Functions ==========\ndef read_code_file(file):\n    return file.read().decode(\"utf-8\")\n\ndef build_prompt(task, code=None, user_prompt=None):\n    if user_prompt:\n        return f\"{user_prompt}\\n\\n```python\\n{code}\\n```\" if code else user_prompt\n    elif task == \"Explain\":\n        return f\"Explain what this code does:\\n\\n```python\\n{code}\\n```\"\n    elif task == \"Refactor\":\n        return f\"\"\"Please refactor the following code to improve clarity, maintainability, and performance. Provide only the revised code with inline comments and no additional explanation:\n\n```python\n{code}\n```\"\"\"\"\"\n    elif task == \"Debug\":\n        return f\"Identify and fix bugs in the following code:\\n\\n```python\\n{code}\\n```\"\n    return \"\"\n\ndef save_output(input_data, result):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    out_file = os.path.join(FILES_DIR, f\"output_{timestamp}.json\")\n    with open(out_file, \"w\") as f:\n        json.dump({\"input\": input_data, \"response\": result}, f, indent=2)\n\ndef get_base64_image(image_path):\n    with open(image_path, \"rb\") as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\n# ========== Streamlit UI ==========\n# Logo and Title Header\nlogo_path = os.path.join(FILES_DIR, \"logo.jpg\")\nif os.path.exists(logo_path):\n    logo_base64 = get_base64_image(logo_path)\n    st.markdown(\n        f\"\"\"\n        <div style='display: flex; align-items: center; gap: 1rem; margin-bottom: 1.5rem;'>\n            <img src='data:image/png;base64,{logo_base64}' style='vertical-align: middle;' width='50'>\n            <h1 style='margin: 0; font-family: Montserrat, sans-serif; color: #2D2042;'>Local Code Assistant with Codellama:7b-instruct</h1>\n        </div>\n        \"\"\",\n        unsafe_allow_html=True\n    )\nelse:\n    st.title(\"Local Code Assistant with Codellama:7b-instruct\")\n\nst.sidebar.header(\"Interaction Mode\")\nmode = st.sidebar.radio(\"Choose a mode:\", [\"Structured\", \"Direct Prompt\"])\nsave_toggle = st.sidebar.checkbox(\"Save output to file\", value=False)\nst.sidebar.markdown(\"---\")\ndef_prompt = \"\"\"\nYou are a helpful and concise code assistant. Your goal is to analyze user-submitted code and respond based on the task type.\n\n- If explaining, be clear and beginner-friendly.\n- If refactoring, improve readability and performance with brief justifications. Output the improved code directly.\n- If debugging, explain the issues before showing the corrected version.\n\nRespond using markdown formatting when appropriate. Only output code inside triple backticks.\n\"\"\"\n# --- Styled System Prompt Section ---\nst.sidebar.markdown(\"\"\"\n    <div style='border: 1px solid #e6e6e6; padding: 12px; border-radius: 8px; background-color: #f9f9f9; margin-top: 10px; margin-bottom: 20px;'>\n        <strong style='color: #2D2042;'>System Prompt</strong><br>\n        <small style='color: #666;'>Customize how the model behaves. Use markdown-friendly formatting.</small>\n    </div>\n\"\"\", unsafe_allow_html=True)\n\ncustom_sys_prompt = st.sidebar.text_area(\"\", value=def_prompt, height=180)\n\n# --- Upload or paste code ---\nuploaded_file = st.file_uploader(\"Upload a code file:\", type=[\"py\", \"js\", \"java\"])\ncode_input = st.text_area(\"Or paste your code here:\")\n\ncode = \"\"\nif uploaded_file:\n    code = read_code_file(uploaded_file)\n    st.success(\"Code loaded from uploaded file.\")\nelif code_input.strip():\n    code = code_input\n\n# --- Prompt Execution ---\nif mode == \"Structured\":\n    task = st.radio(\"What do you want to do?\", [\"Explain\", \"Refactor\", \"Debug\"])\n    if st.button(\"Run Analysis\"):\n        if not code:\n            st.error(\"Please upload or enter some code.\")\n        else:\n            final_prompt = build_prompt(task, code)\n            response = llm.invoke([\n                SystemMessage(content=custom_sys_prompt),\n                HumanMessage(content=final_prompt)\n            ])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"task\": task, \"code\": code}, response.content)\n            \n\nelif mode == \"Direct Prompt\":\n    user_prompt = st.text_area(\"Enter your custom prompt:\")\n    if st.button(\"Run Prompt\"):\n        if not user_prompt.strip():\n            st.error(\"Please enter a prompt.\")\n        else:\n            final_prompt = build_prompt(None, code if code else None, user_prompt)\n            response = llm.invoke([HumanMessage(content=final_prompt)])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"prompt\": user_prompt, \"code\": code}, response.content)\n            \n\n# Footer Branding\nst.markdown(\"\"\"\n    <hr style='margin-top: 3rem;'>\n    <div style='text-align: center; color: #2D2042;'>Smarter Paths Forward</div>\n\"\"\", unsafe_allow_html=True)\n\n\n"
  },
  "response": "The code you provided is a Streamlit app that allows users to interact with a local instance of the Codellama model, which is a language understanding model designed for natural language processing tasks. The app has several features, including a file uploader for users to upload their Python files, a text area for users to paste in their code, a radio button for users to choose between \"Structured\" and \"Direct Prompt\" modes, a text area for users to enter a custom prompt, and a button for users to run the analysis.\n\nThe app also has a sidebar with several settings that allow users to customize how the model behaves, such as whether to save the output to a file. The app uses the `read_code_file` function to read the code from the uploaded file or the text area, and then passes it to the Codellama model for analysis using the `invoke` method.\n\nThe `build_prompt` function is used to build the prompt that will be sent to the Codellama model based on the user's choices. It takes three arguments: the task (Explain, Refactor, or Debug), a code string (optional), and a user-provided prompt (also optional). The function returns the final prompt in the form of a `SystemMessage` object that includes the custom system prompt if specified by the user.\n\nThe `save_output` function is used to save the output of the model to a file. It takes three arguments: the input data (a dictionary with mode, task, and code keys), the response from the Codellama model, and the toggle for saving the output to a file. The function saves the output to a JSON file in the `files` directory with a timestamped filename.\n\nOverall, this app provides a convenient way for users to interact with the Codellama model and get insights into their code based on the task type they choose."
}