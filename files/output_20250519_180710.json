{
  "input": {
    "mode": "Direct Prompt",
    "prompt": "Can you convert this code to C++",
    "code": "import os\nimport streamlit as st\nfrom langchain_ollama import ChatOllama\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom datetime import datetime\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\n# ========== Setup Paths ==========\nBASE_DIR = os.path.expanduser(\"~/myworkspace/utilities/code-demo\")\nFILES_DIR = os.path.join(BASE_DIR, \"files\")\nos.makedirs(FILES_DIR, exist_ok=True)\n\n# ========== Initialize Model ==========\nllm = ChatOllama(model=\"codellama:7b-instruct\")\n\n# ========== Custom Styling ==========\nst.markdown(\"\"\"\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap');\n\n    html, body, [class*=\"css\"]  {\n        font-family: 'Montserrat', sans-serif;\n    }\n\n    h1, h2, h3, h4, h5 {\n        color: #2D2042;\n    }\n\n    .stButton > button {\n        background-color: #60B5E5 !important;\n        color: white !important;\n        font-weight: 600;\n        border-radius: 8px;\n    }\n\n    .stSidebar h1, .stSidebar h2, .stSidebar h3, .stSidebar h4, .stSidebar h5 {\n        color: #60B5E5 !important;\n    }\n\n    section[data-testid=\"stFileUploader\"] > div {\n        box-shadow: 0px 1px 5px rgba(0, 0, 0, 0.05);\n        border-radius: 8px;\n        padding: 1rem;\n    }\n\n    #MainMenu {visibility: hidden;}\n    /* footer {visibility: hidden;} */\n    /* header {visibility: hidden;} */\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# ========== Helper Functions ==========\ndef read_code_file(file):\n    return file.read().decode(\"utf-8\")\n\ndef build_prompt(task, code=None, user_prompt=None):\n    if user_prompt:\n        return f\"{user_prompt}\\n\\n```python\\n{code}\\n```\" if code else user_prompt\n    elif task == \"Explain\":\n        return f\"Explain what this code does:\\n\\n```python\\n{code}\\n```\"\n    elif task == \"Refactor\":\n        return f\"\"\"Please refactor the following code to improve clarity, maintainability, and performance. Provide only the revised code with inline comments and no additional explanation:\n\n```python\n{code}\n```\"\"\"\"\"\n    elif task == \"Debug\":\n        return f\"Identify and fix bugs in the following code:\\n\\n```python\\n{code}\\n```\"\n    return \"\"\n\ndef save_output(input_data, result):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    out_file = os.path.join(FILES_DIR, f\"output_{timestamp}.json\")\n    with open(out_file, \"w\") as f:\n        json.dump({\"input\": input_data, \"response\": result}, f, indent=2)\n\ndef get_base64_image(image_path):\n    with open(image_path, \"rb\") as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\n# ========== Streamlit UI ==========\n# Logo and Title Header\nlogo_path = os.path.join(FILES_DIR, \"logo.jpg\")\nif os.path.exists(logo_path):\n    logo_base64 = get_base64_image(logo_path)\n    st.markdown(\n        f\"\"\"\n        <div style='display: flex; align-items: center; gap: 1rem; margin-bottom: 1.5rem;'>\n            <img src='data:image/png;base64,{logo_base64}' style='vertical-align: middle;' width='50'>\n            <h1 style='margin: 0; font-family: Montserrat, sans-serif; color: #2D2042;'>Local Code Assistant with Codellama:7b-instruct</h1>\n        </div>\n        \"\"\",\n        unsafe_allow_html=True\n    )\nelse:\n    st.title(\"Local Code Assistant with Codellama:7b-instruct\")\n\nst.sidebar.header(\"Interaction Mode\")\nmode = st.sidebar.radio(\"Choose a mode:\", [\"Structured\", \"Direct Prompt\"])\nsave_toggle = st.sidebar.checkbox(\"Save output to file\", value=False)\nst.sidebar.markdown(\"---\")\ndef_prompt = \"\"\"\nYou are a helpful and concise code assistant. Your goal is to analyze user-submitted code and respond based on the task type.\n\n- If explaining, be clear and beginner-friendly.\n- If refactoring, improve readability and performance with brief justifications. Output the improved code directly.\n- If debugging, explain the issues before showing the corrected version.\n\nRespond using markdown formatting when appropriate. Only output code inside triple backticks.\n\"\"\"\n# --- Styled System Prompt Section ---\nst.sidebar.markdown(\"\"\"\n    <div style='border: 1px solid #e6e6e6; padding: 12px; border-radius: 8px; background-color: #f9f9f9; margin-top: 10px; margin-bottom: 20px;'>\n        <strong style='color: #2D2042;'>System Prompt</strong><br>\n        <small style='color: #666;'>Customize how the model behaves. Use markdown-friendly formatting.</small>\n    </div>\n\"\"\", unsafe_allow_html=True)\n\ncustom_sys_prompt = st.sidebar.text_area(\"\", value=def_prompt, height=180)\n\n# --- Upload or paste code ---\nuploaded_file = st.file_uploader(\"Upload a code file:\", type=[\"py\", \"js\", \"java\"])\ncode_input = st.text_area(\"Or paste your code here:\")\n\ncode = \"\"\nif uploaded_file:\n    code = read_code_file(uploaded_file)\n    st.success(\"Code loaded from uploaded file.\")\nelif code_input.strip():\n    code = code_input\n\n# --- Prompt Execution ---\nif mode == \"Structured\":\n    task = st.radio(\"What do you want to do?\", [\"Explain\", \"Refactor\", \"Debug\"])\n    if st.button(\"Run Analysis\"):\n        if not code:\n            st.error(\"Please upload or enter some code.\")\n        else:\n            final_prompt = build_prompt(task, code)\n            response = llm.invoke([\n                SystemMessage(content=custom_sys_prompt),\n                HumanMessage(content=final_prompt)\n            ])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"task\": task, \"code\": code}, response.content)\n            \n\nelif mode == \"Direct Prompt\":\n    user_prompt = st.text_area(\"Enter your custom prompt:\")\n    if st.button(\"Run Prompt\"):\n        if not user_prompt.strip():\n            st.error(\"Please enter a prompt.\")\n        else:\n            final_prompt = build_prompt(None, code if code else None, user_prompt)\n            response = llm.invoke([HumanMessage(content=final_prompt)])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"prompt\": user_prompt, \"code\": code}, response.content)\n            \n\n# Footer Branding\nst.markdown(\"\"\"\n    <hr style='margin-top: 3rem;'>\n    <div style='text-align: center; color: #2D2042;'>Smarter Paths Forward</div>\n\"\"\", unsafe_allow_html=True)\n\n\n"
  },
  "response": "\nThis Python code uses the Streamlit library to create a user interface for generating responses using a chatbot model. The code defines several functions and variables, including:\n\n* `BASE_DIR`: The directory where files are stored.\n* `FILES_DIR`: The directory where output files are saved.\n* `llm`: An instance of the `ChatOllama` class, which is used to interact with the chatbot model.\n* `custom_sys_prompt`: A text area where users can enter custom system prompts for the chatbot.\n* `uploaded_file`: The file uploader component that allows users to select a code file.\n* `code_input`: The text area where users can enter their code manually.\n* `task`: A radio button that allows users to choose between three tasks: explaining, refactoring, and debugging.\n* `final_prompt`: A variable that contains the final prompt sent to the chatbot model for analysis.\n* `response`: The response returned by the chatbot model.\n* `save_output`: A function that saves the output of the chatbot model to a file.\n* `get_base64_image`: A function that reads an image file and returns its base64-encoded representation.\n\nThe code then uses Streamlit's `st` object to create a user interface with several components, including:\n\n1. A logo and title header using the custom CSS styles defined in the `custom_sys_prompt` text area.\n2. An interaction mode selector that allows users to choose between two modes: structured and direct prompts.\n3. A file uploader component for selecting a code file.\n4. A text area where users can enter their code manually.\n5. Radio buttons that allow users to select one of three tasks: explaining, refactoring, or debugging.\n6. A button that runs the analysis when clicked.\n7. A response section that displays the output of the chatbot model in markdown format.\n8. A footer with branding information.\n\nThe code also defines several helper functions, including `read_code_file`, `build_prompt`, and `save_output`. These functions are used to read a code file, build the prompt for the chatbot model, and save the output of the analysis to a file."
}