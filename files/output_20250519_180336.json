{
  "input": {
    "mode": "Structured",
    "task": "Debug",
    "code": "import os\nimport streamlit as st\nfrom langchain_ollama import ChatOllama\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom datetime import datetime\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\n# ========== Setup Paths ==========\nBASE_DIR = os.path.expanduser(\"~/myworkspace/utilities/code-demo\")\nFILES_DIR = os.path.join(BASE_DIR, \"files\")\nos.makedirs(FILES_DIR, exist_ok=True)\n\n# ========== Initialize Model ==========\nllm = ChatOllama(model=\"codellama:7b-instruct\")\n\n# ========== Custom Styling ==========\nst.markdown(\"\"\"\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap');\n\n    html, body, [class*=\"css\"]  {\n        font-family: 'Montserrat', sans-serif;\n    }\n\n    h1, h2, h3, h4, h5 {\n        color: #2D2042;\n    }\n\n    .stButton > button {\n        background-color: #60B5E5 !important;\n        color: white !important;\n        font-weight: 600;\n        border-radius: 8px;\n    }\n\n    .stSidebar h1, .stSidebar h2, .stSidebar h3, .stSidebar h4, .stSidebar h5 {\n        color: #60B5E5 !important;\n    }\n\n    section[data-testid=\"stFileUploader\"] > div {\n        box-shadow: 0px 1px 5px rgba(0, 0, 0, 0.05);\n        border-radius: 8px;\n        padding: 1rem;\n    }\n\n    #MainMenu {visibility: hidden;}\n    /* footer {visibility: hidden;} */\n    /* header {visibility: hidden;} */\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# ========== Helper Functions ==========\ndef read_code_file(file):\n    return file.read().decode(\"utf-8\")\n\ndef build_prompt(task, code=None, user_prompt=None):\n    if user_prompt:\n        return f\"{user_prompt}\\n\\n```python\\n{code}\\n```\" if code else user_prompt\n    elif task == \"Explain\":\n        return f\"Explain what this code does:\\n\\n```python\\n{code}\\n```\"\n    elif task == \"Refactor\":\n        return f\"\"\"Please refactor the following code to improve clarity, maintainability, and performance. Provide only the revised code with inline comments and no additional explanation:\n\n```python\n{code}\n```\"\"\"\"\"\n    elif task == \"Debug\":\n        return f\"Identify and fix bugs in the following code:\\n\\n```python\\n{code}\\n```\"\n    return \"\"\n\ndef save_output(input_data, result):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    out_file = os.path.join(FILES_DIR, f\"output_{timestamp}.json\")\n    with open(out_file, \"w\") as f:\n        json.dump({\"input\": input_data, \"response\": result}, f, indent=2)\n\ndef get_base64_image(image_path):\n    with open(image_path, \"rb\") as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\n# ========== Streamlit UI ==========\n# Logo and Title Header\nlogo_path = os.path.join(FILES_DIR, \"logo.jpg\")\nif os.path.exists(logo_path):\n    logo_base64 = get_base64_image(logo_path)\n    st.markdown(\n        f\"\"\"\n        <div style='display: flex; align-items: center; gap: 1rem; margin-bottom: 1.5rem;'>\n            <img src='data:image/png;base64,{logo_base64}' style='vertical-align: middle;' width='50'>\n            <h1 style='margin: 0; font-family: Montserrat, sans-serif; color: #2D2042;'>Local Code Assistant with Codellama:7b-instruct</h1>\n        </div>\n        \"\"\",\n        unsafe_allow_html=True\n    )\nelse:\n    st.title(\"Local Code Assistant with Codellama:7b-instruct\")\n\nst.sidebar.header(\"Interaction Mode\")\nmode = st.sidebar.radio(\"Choose a mode:\", [\"Structured\", \"Direct Prompt\"])\nsave_toggle = st.sidebar.checkbox(\"Save output to file\", value=False)\nst.sidebar.markdown(\"---\")\ndef_prompt = \"\"\"\nYou are a helpful and concise code assistant. Your goal is to analyze user-submitted code and respond based on the task type.\n\n- If explaining, be clear and beginner-friendly.\n- If refactoring, improve readability and performance with brief justifications. Output the improved code directly.\n- If debugging, explain the issues before showing the corrected version.\n\nRespond using markdown formatting when appropriate. Only output code inside triple backticks.\n\"\"\"\n# --- Styled System Prompt Section ---\nst.sidebar.markdown(\"\"\"\n    <div style='border: 1px solid #e6e6e6; padding: 12px; border-radius: 8px; background-color: #f9f9f9; margin-top: 10px; margin-bottom: 20px;'>\n        <strong style='color: #2D2042;'>System Prompt</strong><br>\n        <small style='color: #666;'>Customize how the model behaves. Use markdown-friendly formatting.</small>\n    </div>\n\"\"\", unsafe_allow_html=True)\n\ncustom_sys_prompt = st.sidebar.text_area(\"\", value=def_prompt, height=180)\n\n# --- Upload or paste code ---\nuploaded_file = st.file_uploader(\"Upload a code file:\", type=[\"py\", \"js\", \"java\"])\ncode_input = st.text_area(\"Or paste your code here:\")\n\ncode = \"\"\nif uploaded_file:\n    code = read_code_file(uploaded_file)\n    st.success(\"Code loaded from uploaded file.\")\nelif code_input.strip():\n    code = code_input\n\n# --- Prompt Execution ---\nif mode == \"Structured\":\n    task = st.radio(\"What do you want to do?\", [\"Explain\", \"Refactor\", \"Debug\"])\n    if st.button(\"Run Analysis\"):\n        if not code:\n            st.error(\"Please upload or enter some code.\")\n        else:\n            final_prompt = build_prompt(task, code)\n            response = llm.invoke([\n                SystemMessage(content=custom_sys_prompt),\n                HumanMessage(content=final_prompt)\n            ])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"task\": task, \"code\": code}, response.content)\n            \n\nelif mode == \"Direct Prompt\":\n    user_prompt = st.text_area(\"Enter your custom prompt:\")\n    if st.button(\"Run Prompt\"):\n        if not user_prompt.strip():\n            st.error(\"Please enter a prompt.\")\n        else:\n            final_prompt = build_prompt(None, code if code else None, user_prompt)\n            response = llm.invoke([HumanMessage(content=final_prompt)])\n            st.subheader(\"Model Response\")\n            st.code(response.content, language='markdown')\n            if save_toggle:\n                save_output({\"mode\": mode, \"prompt\": user_prompt, \"code\": code}, response.content)\n            \n\n# Footer Branding\nst.markdown(\"\"\"\n    <hr style='margin-top: 3rem;'>\n    <div style='text-align: center; color: #2D2042;'>Smarter Paths Forward</div>\n\"\"\", unsafe_allow_html=True)\n\n\n"
  },
  "response": "This code is a Streamlit application that allows users to input Python code and get responses from the Codellama model. The app has several features, including:\n\n* File uploader for uploading Python code files (.py extension) or pasting Python code directly in the app.\n* Radio button menu for choosing between two interaction modes: \"Structured\" (with tasks like Explain, Refactor, and Debug) and \"Direct Prompt\" (where users can enter a custom prompt).\n* A sidebar with options for customizing the system prompt used by the model.\n* A text area for entering a custom user prompt.\n* Buttons for running analysis and direct prompts.\n* Responses from the model in markdown format.\n* An option to save output to a file (if enabled).\n* Footer with branding information.\n\nThe code includes several helper functions, such as read_code_file(), build_prompt(), save_output(), and get_base64_image(). The read_code_file() function reads a Python code file and returns its contents as a string. The build_prompt() function builds the final prompt to be sent to the model based on the task and user input (or custom prompt). The save_output() function saves the output from the model to a JSON file. The get_base64_image() function converts an image file to base64-encoded data for embedding in HTML.\n\nOverall, this code is well-organized, with clear and concise comments that explain each section of the code. However, there are some areas where the code can be improved:\n\n* The app does not have any error handling for invalid file uploads or empty prompts. It would be good to add some basic error handling to ensure a better user experience.\n* The custom system prompt in the sidebar is not clearly labeled, which may make it difficult for users to understand its purpose and how to use it effectively.\n* The save output option could be improved by allowing users to specify the filename and directory for the saved file, rather than relying on the default behavior of saving to a fixed location.\n* Finally, adding more features or customization options to the app would make it more versatile and useful for a wider range of users. For example, users could benefit from the ability to upload other types of files (e.g., JavaScript code), generate visualizations or diagrams of the code, or provide feedback on the model's performance."
}